{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0 IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mathe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import warnings\n",
    "import sweetviz\n",
    "import IPython\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import pandas            as pd\n",
    "import numpy             as np\n",
    "import lightgbm          as lgb\n",
    "import xgboost           as xgb\n",
    "import seaborn           as sns\n",
    "\n",
    "from sklearn.linear_model    import LogisticRegression\n",
    "from category_encoders       import TargetEncoder\n",
    "from sklearn.preprocessing   import OneHotEncoder\n",
    "from sklearn.ensemble        import ExtraTreesClassifier, RandomForestClassifier, StackingClassifier, HistGradientBoostingClassifier\n",
    "from scipy                   import stats\n",
    "from unidecode               import unidecode\n",
    "from catboost                import CatBoostClassifier\n",
    "from skopt                   import gp_minimize\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing   import RobustScaler, MinMaxScaler\n",
    "from imblearn                import over_sampling, under_sampling\n",
    "from sklearn.neural_network  import MLPClassifier\n",
    "from sklearn.naive_bayes     import GaussianNB\n",
    "from sklearn                 import svm\n",
    "from sklearn.impute          import KNNImputer\n",
    "from sklearn.neighbors       import KNeighborsClassifier\n",
    "\n",
    "from sklearn                 import model_selection   as ms\n",
    "from sklearn                 import metrics           as m\n",
    "from matplotlib              import pyplot            as plt\n",
    "from imblearn                import combine           as co"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 AUX FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "def ml_error( model_name, ytest, yhat ):\n",
    "    f1 = m.f1_score( ytest, yhat )\n",
    "    precision = m.precision_score( ytest, yhat )\n",
    "    recall = m.recall_score( ytest, yhat )\n",
    "\n",
    "    return pd.DataFrame( {'Model name': model_name,\n",
    "                          'F1': f1,\n",
    "                          'Precision': precision,\n",
    "                          'Recall': recall  }, index=[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 READ DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users = pd.read_csv( \"Dataset_model.csv\" )\n",
    "df_questions = pd.read_csv( \"subjects_questions.csv\" )\n",
    "\n",
    "df_users = df_users.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "\n",
    "df_questions.dropna( inplace=True )\n",
    "df_questions['subject_id'] = df_questions['subject_id'].astype(str)\n",
    "df_questions = df_questions.groupby(['novo_question_id']).agg({'subject_id': ', '.join}).reset_index()\n",
    "df_users = df_users.merge( df_questions, how='left', left_on='novo_question_id', right_on='novo_question_id' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxa_acerto = pd.read_csv( 'features/taxa_acertos.csv' )\n",
    "df_taxa_acerto_knowledge = pd.read_csv( 'features/taxa_acerto_knowledge.csv' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dados_estado = pd.read_csv( \"Dados Estado Simplificado.csv\" )\n",
    "df_dados_municipio = pd.read_excel( \"Dados Município Simplificado.xlsx\", sheet_name=\"Dados\" )\n",
    "df_pib = pd.read_excel( \"PIB Municípios.xlsx\" )\n",
    "df_ibge = pd.read_excel( \"IBGE.xlsx\", sheet_name=\"Planilha1\" )\n",
    "\n",
    "df_users['region'] = df_users['region'].apply( lambda x: unidecode(x).upper() if type(x) is not float else x )\n",
    "\n",
    "# merge public datasets\n",
    "df_dados_estado.rename( columns={ 'População':'populacao_estado','Matrículas':'matriculas_estado',\n",
    "                                  'Categoria do Quadrante':'categoria_quadrante_estado','Insumos do Ioeb':'insumos_ioeb_estado',\n",
    "                                  'Ioeb':'ioeb_estado','Resultados do Ioeb':'resultados_ioeb_estado'}, inplace=True)\n",
    "                                  \n",
    "df_dados_municipio.rename( columns={ 'População':'populacao_cidade','Total de Matrículas':'matriculas_cidade',\n",
    "                                  'Categoria do Quadrante':'categoria_quadrante_cidade','Insumos do Ioeb':'insumos_ioeb_cidade',\n",
    "                                  'Ioeb':'ioeb_cidade','Resultados do Ioeb':'resultados_ioeb_cidade','Região':'regiao' }, inplace=True)\n",
    "\n",
    "for col in df_dados_estado.columns:\n",
    "    if df_dados_estado[f'{col}'].dtypes != \"int64\" and df_dados_estado[f'{col}'].dtypes != \"float64\":\n",
    "        df_dados_estado[f'{col}'] = df_dados_estado[f'{col}'].apply(lambda x: x.replace( \",\", \"\" ) if type(x) is not float else x )\n",
    "        df_dados_estado[f'{col}'] = df_dados_estado[f'{col}'].apply(lambda x: x.replace( \".\", \"\" ) if type(x) is not float else x )\n",
    "\n",
    "for col in df_dados_municipio.columns:\n",
    "    if df_dados_municipio[f'{col}'].dtypes != \"int64\" and type(df_dados_municipio[f'{col}']) != \"float64\":\n",
    "        df_dados_municipio[f'{col}'] = df_dados_municipio[f'{col}'].apply(lambda x: x.replace( \",\", \"\" ) if type(x) is not float else x )\n",
    "        df_dados_municipio[f'{col}'] = df_dados_municipio[f'{col}'].apply(lambda x: x.replace( \".\", \"\" ) if type(x) is not float else x )\n",
    "\n",
    "# merge df users com df dados estado\n",
    "df_dados_estado['Estado'] = df_dados_estado['Estado'].apply( lambda x: unidecode(x).upper() )\n",
    "df_users['region'] = df_users['region'].apply( lambda x: unidecode(x).upper() if type(x) is not float else x )\n",
    "df_users['city'] = df_users['city'].apply( lambda x: unidecode(x).upper() if type(x) is not float else x )\n",
    "df_users['city'] = df_users['city'].apply( lambda x: x.replace( \"'\", \"\" ) if type(x) is not float else x )\n",
    "df_users.loc[df_users.city == 'FEDERAL DISTRICT', 'city'] = 'BRASILIA'\n",
    "df_users.loc[df_users.city == 'BRASILIA', 'region'] = 'DISTRITO FEDERAL'\n",
    "\n",
    "df_users = df_users.merge( df_dados_estado, how='left', left_on='region', right_on='Estado' )\n",
    "df_users.drop( columns=['Estado','Ano do Ioeb','Código da UF','Unidade da Federação','Região'], inplace=True )\n",
    "\n",
    "# merge df users com df dados municipio\n",
    "df_dados_municipio['Nome do Município'] = df_dados_municipio['Nome do Município'].apply( lambda x: unidecode(x).upper() )\n",
    "df_dados_municipio['estado'] = df_dados_municipio['estado'].apply( lambda x: unidecode(x).upper() )\n",
    "\n",
    "df_users = df_users.merge( df_dados_municipio, how='left', left_on=['city','region'], right_on=['Nome do Município','estado'] )\n",
    "df_users.drop( columns=['Código do Município','Código da UF','Unidade da Federação','Nome do Município','Ano do Ioeb','estado'], inplace=True )\n",
    "\n",
    "features_to_float = ['ioeb_estado', 'insumos_ioeb_estado', 'resultados_ioeb_estado', 'populacao_estado', \n",
    "'ioeb_cidade', 'insumos_ioeb_cidade', 'resultados_ioeb_cidade', 'matriculas_cidade', 'populacao_cidade']\n",
    "df_users[features_to_float] = df_users[features_to_float].astype(float)\n",
    "\n",
    "df_pib['Município'] = df_pib['Município'].apply( lambda x: x.split('(')[0] )\n",
    "df_pib['Município'] = df_pib['Município'].apply( lambda x: unidecode(x).upper().strip() )\n",
    "df_pib['Município'] = df_pib['Município'].apply( lambda x: x.replace( \"'\", \"\" ) )\n",
    "df_pib['estado'] = df_pib['estado'].apply( lambda x: unidecode(x).upper().strip() )\n",
    "\n",
    "\n",
    "df_users = df_users.merge( df_pib, how='left', left_on=['city','region'], right_on=['Município','estado'] )\n",
    "df_users.drop( columns=['Município','estado'], inplace=True )\n",
    "\n",
    "df_ibge = df_ibge[['Município','estado','IDHM','IDHM_E','IDHM_L','IDHM_R','I_ESCOLARIDADE','I_FREQ_PROP','TRABPUB','RENOCUP','GINI','T_FLSUPER',\n",
    "'T_ANALF18M','E_ANOSESTUDO','ESPVIDA','T_FBMED', 'T_FBSUPER', 'RDPC','PESO1824','T_MED18A24','T_FREQFUND1824']]\n",
    "df_ibge['Município'] = df_ibge['Município'].apply( lambda x: unidecode(x).upper().strip() )\n",
    "df_ibge['Município'] = df_ibge['Município'].apply( lambda x: x.replace( \"'\", \"\" ) )\n",
    "df_ibge['estado'] = df_ibge['estado'].apply( lambda x: unidecode(x).upper().strip() )\n",
    "\n",
    "df_users = df_users.merge( df_ibge, how='left', left_on=['city','region'], right_on=['Município','estado'] )\n",
    "df_users.drop( columns=['Município','estado'], inplace=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users = df_users[['novo_user_id', 'city', 'country', 'device', 'device_type', 'os',\n",
    "       'platform', 'region', 'gpcarrers', 'gpcollegetype', 'gpdegreecourse',\n",
    "       'gppreviousexperience', 'gpschooltype', 'gpsegment', 'gpsource_project',\n",
    "       'acertou', 'created_at', 'row', 'commented_by_professor', 'difficulty',\n",
    "       'discipline_id', 'examining_board_id', 'institute_id',\n",
    "       'knowledge_area_id', 'modality_id', 'nullified', 'outdated',\n",
    "       'product_id', 'publication_year', 'right_answer', 'scholarity_id',\n",
    "       'novo_question_id', 'subject_id','PIB','TRABPUB']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('{}'.format(len(df_users['discipline_id'].unique())))\n",
    "# print('{}'.format(len(df_users['examining_board_id'].unique())))\n",
    "# print('{}'.format(len(df_users['institute_id'].unique())))\n",
    "# print('{}'.format(len(df_users['knowledge_area_id'].unique())))\n",
    "# print('{}'.format(len(df_users['subject_id'].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 DATA DESCRIPTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df users rows: 2000000\n",
      "df users columns: 35\n"
     ]
    }
   ],
   "source": [
    "print( f\"df users rows: {df_users.shape[0]}\" )\n",
    "print( f\"df users columns: {df_users.shape[1]}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Check NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_users['os'] = df_users['os'].fillna( df_users['os'].mode()[0] )\n",
    "# df_users['gpcarrers'] = df_users['gpcarrers'].fillna( df_users['gpcarrers'].mode()[0] )\n",
    "# df_users['discipline_id'] = df_users['discipline_id'].fillna( df_users['discipline_id'].mode()[0] )\n",
    "# df_users['examining_board_id'] = df_users['examining_board_id'].fillna( df_users['examining_board_id'].mode()[0] )\n",
    "# df_users['institute_id'] = df_users['institute_id'].fillna( df_users['institute_id'].mode()[0] )\n",
    "# df_users['knowledge_area_id'] = df_users['knowledge_area_id'].fillna( df_users['knowledge_area_id'].mode()[0] )\n",
    "# df_users['publication_year'] = df_users['publication_year'].fillna( df_users['publication_year'].mode()[0] )\n",
    "# df_users['PIB'] = df_users['PIB'].fillna( df_users['PIB'].mean() )\n",
    "# df_users['TRABPUB'] = df_users['TRABPUB'].fillna( df_users['TRABPUB'].mean() )\n",
    "# df_users['subject_id'] = df_users['subject_id'].fillna( df_users['subject_id'].mode()[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users['created_at'] = pd.to_datetime(df_users['created_at'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_users = df_users.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_users = df2_users.merge( df_taxa_acerto, how='left', right_on=['novo_user_id','difficulty'], left_on=['novo_user_id','difficulty'])\n",
    "df2_users = df2_users.merge( df_taxa_acerto_knowledge, how='left', right_on=['novo_user_id','knowledge_area_id'], left_on=['novo_user_id','knowledge_area_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_users = df2_users['novo_user_id'].unique()\n",
    "# difficulty_list = df2_users['difficulty'].unique()\n",
    "# df2_users['taxa_acerto'] = math.nan\n",
    "\n",
    "# for user in list_users:\n",
    "#     df_user = df2_users.loc[df2_users['novo_user_id'] == user ]\n",
    "#     print(user)\n",
    "    \n",
    "#     for difficulty in difficulty_list:\n",
    "#         taxa_acerto = df_user.loc[df_user['difficulty'] == difficulty, 'acertou'].mean()\n",
    "#         df2_users.loc[(df2_users['novo_user_id'] == user) & (df2_users['difficulty'] == difficulty), 'taxa_acerto'] = taxa_acerto\n",
    "#         print(taxa_acerto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_taxa_acertos = df2_users[['novo_user_id','difficulty','taxa_acerto']]\n",
    "# df_taxa_acertos.drop_duplicates(subset=['novo_user_id','difficulty'], inplace=True )\n",
    "# df_taxa_acertos.reset_index(drop=True, inplace=True)\n",
    "# df_taxa_acertos.to_csv('features/taxa_acertos.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_users = df2_users['novo_user_id'].unique()\n",
    "# knowledge_list = df2_users['knowledge_area_id'].unique()\n",
    "# df2_users['taxa_acerto_knowledge'] = math.nan\n",
    "\n",
    "# for user in list_users:\n",
    "#     df_user = df2_users.loc[df2_users['novo_user_id'] == user ]\n",
    "#     print(user)\n",
    "    \n",
    "#     for knowledge in knowledge_list:\n",
    "#         taxa_acerto_knowledge = df_user.loc[df_user['knowledge_area_id'] == knowledge, 'acertou'].mean()\n",
    "#         df2_users.loc[(df2_users['novo_user_id'] == user) & (df2_users['knowledge_area_id'] == knowledge), 'taxa_acerto_knowledge'] = taxa_acerto_knowledge\n",
    "#         if not pd.isnull(taxa_acerto_knowledge):\n",
    "#             print(taxa_acerto_knowledge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_taxa_acertos_knowledge = df2_users[['novo_user_id','knowledge_area_id','taxa_acerto_knowledge']]\n",
    "# df_taxa_acertos_knowledge.drop_duplicates(subset=['novo_user_id','knowledge_area_id'], inplace=True )\n",
    "# df_taxa_acertos_knowledge.reset_index(drop=True, inplace=True)\n",
    "# df_taxa_acertos_knowledge.to_csv('features/taxa_acerto_knowledge.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year\n",
    "df2_users['year'] = df2_users['created_at'].dt.year\n",
    "\n",
    "# month\n",
    "df2_users['month'] = df2_users['created_at'].dt.month\n",
    "\n",
    "# day\n",
    "df2_users['day'] = df2_users['created_at'].dt.day\n",
    "\n",
    "# week of year \n",
    "df2_users['week_of_year'] = df2_users['created_at'].dt.isocalendar().week\n",
    "df2_users['week_of_year'] = df2_users['week_of_year'].astype(int)\n",
    "\n",
    "# weekday\n",
    "df2_users['day_of_week'] = df2_users['created_at'].dt.weekday\n",
    "\n",
    "# hour\n",
    "df2_users['hour'] = df2_users['created_at'].dt.hour\n",
    "\n",
    "# minute\n",
    "df2_users['minute'] = df2_users['created_at'].dt.minute\n",
    "\n",
    "# second\n",
    "df2_users['second'] = df2_users['created_at'].dt.second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nivel de concentracao\n",
    "df2_users['concentracao'] = df2_users['row'].apply( lambda x: 'pomodoro1' if x <=10 else \n",
    "                                                              'pomodoro2' if x > 10 and x <= 20 else\n",
    "                                                              'pomodoro3' if x > 20 and x <= 30 else\n",
    "                                                              'pomodoro4' if x > 30 and x <= 40 else \n",
    "                                                              'pomodoro5' if x > 40 and x <= 50 else \n",
    "                                                              'pomodoro6' if x > 50 and x <= 60 else \n",
    "                                                              'pomodoro7' if x > 60 and x <= 70 else \n",
    "                                                              'pomodoro8' if x > 70 and x <= 80 else \n",
    "                                                              'pomodoro9' if x > 80 and x <= 90 else \n",
    "                                                              'pomodoro10' )\n",
    "\n",
    "# letras_recorrentes\n",
    "letras_recorrentes = ['A','E']\n",
    "df2_users['letras_recorrentes'] = df2_users['right_answer'].apply( lambda x: 'recorrente' if x in letras_recorrentes else 'nao recorrente'  )\n",
    "\n",
    "# nivel_dificuldade: 1/2-3/3-5\n",
    "df2_users['nivel_dificuldade'] = df2_users['difficulty'].apply( lambda x: 'Baixa' if x <= 1 else \n",
    "                                                                          'Média' if x > 1 and x <= 3 else\n",
    "                                                                          'Alta' if x > 3 else 'NA' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_users['subject_id'] = df2_users.apply( lambda x: str(x['subject_id']) + ' - ' + str(x['scholarity_id']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 DATA FILTERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Análise Bivariada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0 DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5_users = df2_users.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescaling\n",
    "mms = MinMaxScaler()\n",
    "rs = RobustScaler()\n",
    "\n",
    "df5_users['discipline_id'] = rs.fit_transform( df5_users[['discipline_id']].values )\n",
    "pickle.dump( rs, open( 'encoders/discipline_id_scaler', 'wb') )\n",
    "\n",
    "df5_users['examining_board_id'] = rs.fit_transform( df5_users[['examining_board_id']].values )\n",
    "pickle.dump( rs, open( 'encoders/examining_board_id_scaler', 'wb') )\n",
    "\n",
    "df5_users['institute_id'] = rs.fit_transform( df5_users[['institute_id']].values )\n",
    "pickle.dump( rs, open( 'encoders/institute_id_scaler', 'wb') )\n",
    "\n",
    "df5_users['knowledge_area_id'] = rs.fit_transform( df5_users[['knowledge_area_id']].values )\n",
    "pickle.dump( rs, open( 'encoders/knowledge_area_id_scaler', 'wb') )\n",
    "\n",
    "df5_users['PIB'] = mms.fit_transform( df5_users[['PIB']].values )\n",
    "pickle.dump( mms, open( 'encoders/PIB_scaler', 'wb') )\n",
    "\n",
    "df5_users['TRABPUB'] = mms.fit_transform( df5_users[['TRABPUB']].values )\n",
    "pickle.dump( mms, open( 'encoders/TRABPUB_scaler', 'wb') )\n",
    "\n",
    "df5_users['year'] = mms.fit_transform( df5_users[['year']].values )\n",
    "pickle.dump( mms, open( 'encoders/year_scaler', 'wb') )\n",
    "\n",
    "# df5_users['month'] = mms.fit_transform( df5_users[['month']].values )\n",
    "# pickle.dump( mms, open( 'encoders/month_scaler', 'wb') )\n",
    "\n",
    "# df5_users['day'] = mms.fit_transform( df5_users[['day']].values )\n",
    "# pickle.dump( mms, open( 'encoders/day_scaler', 'wb') )\n",
    "\n",
    "# df5_users['week_of_year'] = mms.fit_transform( df5_users[['week_of_year']].values )\n",
    "# pickle.dump( mms, open( 'encoders/week_of_year_scaler', 'wb') )\n",
    "\n",
    "df5_users['day_of_week'] = mms.fit_transform( df5_users[['day_of_week']].values )\n",
    "pickle.dump( mms, open( 'encoders/day_of_week_scaler', 'wb') )\n",
    "\n",
    "# df5_users['hour'] = mms.fit_transform( df5_users[['hour']].values )\n",
    "# pickle.dump( mms, open( 'encoders/hour_scaler', 'wb') )\n",
    "\n",
    "# df5_users['minute'] = mms.fit_transform( df5_users[['minute']].values )\n",
    "# pickle.dump( mms, open( 'encoders/minute_scaler', 'wb') )\n",
    "\n",
    "# df5_users['second'] = mms.fit_transform( df5_users[['second']].values )\n",
    "# pickle.dump( mms, open( 'encoders/second_scaler', 'wb') )\n",
    "\n",
    "df5_users['publication_year'] = mms.fit_transform( df5_users[['publication_year']].values )\n",
    "pickle.dump( mms, open( 'encoders/publication_year_scaler', 'wb') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target encoder\n",
    "te = TargetEncoder()\n",
    "\n",
    "df5_users['city'] = te.fit_transform( df5_users['city'], df5_users['acertou'] )\n",
    "pickle.dump( te, open( 'encoders/city_encoding', 'wb' ) )\n",
    "\n",
    "df5_users['os'] = te.fit_transform( df5_users['os'], df5_users['acertou'] )\n",
    "pickle.dump( te, open( 'encoders/os_encoding', 'wb' ) )\n",
    "\n",
    "df5_users['gpcarrers'] = te.fit_transform( df5_users['gpcarrers'], df5_users['acertou'] )\n",
    "pickle.dump( te, open( 'encoders/gpcarrers_encoding', 'wb' ) )\n",
    "\n",
    "df5_users['subject_id'] = te.fit_transform( df5_users['subject_id'], df5_users['acertou'] )\n",
    "pickle.dump( te, open( 'encoders/subject_id_encoding', 'wb' ) )\n",
    "\n",
    "# df5_users['country'] = te.fit_transform( df5_users['country'], df5_users['acertou'] )\n",
    "# # pickle.dump( te, open( 'encoders/subject_id_encoding', 'wb' ) )\n",
    "\n",
    "#df5_users['device'] = te.fit_transform( df5_users['device'], df5_users['acertou'] )\n",
    "# pickle.dump( te, open( 'encoders/subject_id_encoding', 'wb' ) )\n",
    "\n",
    "#df5_users['device_type'] = te.fit_transform( df5_users['device_type'], df5_users['acertou'] )\n",
    "# pickle.dump( te, open( 'encoders/subject_id_encoding', 'wb' ) )\n",
    "\n",
    "df5_users['platform'] = te.fit_transform( df5_users['platform'], df5_users['acertou'] )\n",
    "pickle.dump( te, open( 'encoders/platform_encoding', 'wb' ) )\n",
    "\n",
    "# df5_users['region'] = te.fit_transform( df5_users['region'], df5_users['acertou'] )\n",
    "# pickle.dump( te, open( 'encoders/subject_id_encoding', 'wb' ) )\n",
    "\n",
    "# df5_users['gpcollegetype'] = te.fit_transform( df5_users['gpcollegetype'], df5_users['acertou'] )\n",
    "# # pickle.dump( te, open( 'encoders/subject_id_encoding', 'wb' ) )\n",
    "\n",
    "# df5_users['gpdegreecourse'] = te.fit_transform( df5_users['gpdegreecourse'], df5_users['acertou'] )\n",
    "# # pickle.dump( te, open( 'encoders/subject_id_encoding', 'wb' ) )\n",
    "\n",
    "# df5_users['gppreviousexperience'] = te.fit_transform( df5_users['gppreviousexperience'], df5_users['acertou'] )\n",
    "# pickle.dump( te, open( 'encoders/subject_id_encoding', 'wb' ) )\n",
    "\n",
    "# df5_users['gpschooltype'] = te.fit_transform( df5_users['gpschooltype'], df5_users['acertou'] )\n",
    "# # pickle.dump( te, open( 'encoders/subject_id_encoding', 'wb' ) )\n",
    "\n",
    "# df5_users['gpsegment'] = te.fit_transform( df5_users['gpsegment'], df5_users['acertou'] )\n",
    "# # pickle.dump( te, open( 'encoders/subject_id_encoding', 'wb' ) )\n",
    "\n",
    "# df5_users['gpsource_project'] = te.fit_transform( df5_users['gpsource_project'], df5_users['acertou'] )\n",
    "# # pickle.dump( te, open( 'encoders/subject_id_encoding', 'wb' ) )\n",
    "\n",
    "df5_users['right_answer'] = te.fit_transform( df5_users['right_answer'], df5_users['acertou'] )\n",
    "pickle.dump( te, open( 'encoders/right_answer_encoding', 'wb' ) )\n",
    "\n",
    "df5_users['letras_recorrentes'] = te.fit_transform( df5_users['letras_recorrentes'], df5_users['acertou'] )\n",
    "pickle.dump( te, open( 'encoders/letras_recorrentes_encoding', 'wb' ) )\n",
    "\n",
    "df5_users['concentracao'] = te.fit_transform( df5_users['concentracao'], df5_users['acertou'] )\n",
    "pickle.dump( te, open( 'encoders/concentracao_encoding', 'wb' ) )\n",
    "\n",
    "df5_users['nivel_dificuldade'] = te.fit_transform( df5_users['nivel_dificuldade'], df5_users['acertou'] )\n",
    "pickle.dump( te, open( 'encoders/nivel_dificuldade_encoding', 'wb' ) )\n",
    "\n",
    "df5_users['created_at'] = df5_users['created_at'].astype(str)\n",
    "df5_users['created_at'] = te.fit_transform( df5_users['created_at'], df5_users['acertou'] )\n",
    "pickle.dump( te, open( 'encoders/created_at_encoding', 'wb' ) )\n",
    "\n",
    "df5_users.drop( columns=['novo_user_id','novo_question_id'], inplace=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.0 FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6_users = df5_users.copy()\n",
    "\n",
    "# # model\n",
    "# forest = ExtraTreesClassifier( n_jobs=-1 )\n",
    "\n",
    "# # training\n",
    "# x_train_fselection = df6_users.drop(columns=['acertou'])\n",
    "# y_train_fselection = df6_users['acertou'].values\n",
    "\n",
    "# forest.fit( x_train_fselection, y_train_fselection )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importances = forest.feature_importances_\n",
    "# std = np.std( [tree.feature_importances_ for tree in forest.estimators_], axis=0 )\n",
    "# indices = np.argsort( importances )[::-1]\n",
    "\n",
    "# # print the feature ranking\n",
    "# df = pd.DataFrame()\n",
    "\n",
    "# print( 'Feature Ranking:\\n' )\n",
    "# for i, j in zip( x_train_fselection,forest.feature_importances_ ):\n",
    "#     aux = pd.DataFrame( {'feature': i, 'importance': j}, index=[0] )\n",
    "#     df = pd.concat( [df, aux], axis=0 )\n",
    "    \n",
    "# print( df.sort_values( 'importance', ascending=False ) ) \n",
    "\n",
    "# # plot the impurity-based feature importances of the forest\n",
    "# plt.figure(figsize=(10,5))\n",
    "# plt.title( 'Feature importances' )\n",
    "# plt.bar( range( x_train_fselection.shape[1] ), importances[indices], color='r', yerr=std[indices], align='center' )\n",
    "# plt.xticks( range(x_train_fselection.shape[1]), indices )\n",
    "# plt.xlim( [-1, x_train_fselection.shape[1]] )\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.0 MACHINE LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['city','acertou','row','region','device_type','device','country','gpschooltype','gpcollegetype','gpdegreecourse','nullified','outdated','gpsegment','product_id','modality_id',\n",
    "'commented_by_professor','gpsource_project','scholarity_id','gppreviousexperience','difficulty','second','minute','gpcarrers','letras_recorrentes','month','day','week_of_year','hour','concentracao','discipline_id','taxa_acerto','taxa_acerto_knowledge']\n",
    "X = df6_users.drop(columns=cols)\n",
    "Y = df6_users['acertou'].copy() \n",
    "\n",
    "X_train, X_val, y_train, y_val = ms.train_test_split( X, Y, test_size=0.2, random_state=42 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model name</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.79949</td>\n",
       "      <td>0.679576</td>\n",
       "      <td>0.970791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model name       F1  Precision    Recall\n",
       "0   LightGBM  0.79949   0.679576  0.970791"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model\n",
    "model_lgb = lgb.LGBMClassifier( n_estimators=420, max_depth=10, learning_rate=0.0021326894421670327, num_leaves=120, min_child_samples=33, subsample=0.3969072433479774, \n",
    "                                colsample_bytree=0.9971483355092672, n_jobs=-1, random_state=42, subsample_freq=1 ).fit( X_train, y_train )\n",
    "# model_lgb = lgb.LGBMClassifier().fit( X_train, y_train )\n",
    "\n",
    "# prediction\n",
    "yhat_lgb = model_lgb.predict( X_val )\n",
    "\n",
    "# performance\n",
    "model_lgb_results = ml_error( 'LightGBM',  y_val, yhat_lgb )\n",
    "model_lgb_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lgb = lgb.LGBMClassifier( n_estimators=420, max_depth=10, learning_rate=0.0021326894421670327, num_leaves=120, min_child_samples=33, subsample=0.3969072433479774, \n",
    "                                colsample_bytree=0.9971483355092672, n_jobs=-1, random_state=42, subsample_freq=1 ).fit( X, Y )\n",
    "\n",
    "pickle.dump( model_lgb, open( 'model/model_lgb.pkl', 'wb' ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7999300368574012 +/- 0.0004896090824439788\n"
     ]
    }
   ],
   "source": [
    "# cross validation\n",
    "kfold = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "cv = cross_val_score(model_lgb, X, Y, scoring='f1', cv=kfold, n_jobs=-1, error_score='raise' )\n",
    "print(f'{np.mean(cv)} +/- {np.std(cv)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 HistGradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model\n",
    "# model_hist = HistGradientBoostingClassifier().fit( X_train, y_train )\n",
    "\n",
    "# # prediction\n",
    "# yhat_hist = model_hist.predict( X_val )\n",
    "\n",
    "# # performance\n",
    "# model_hist_results = ml_error( 'Hist',  y_val, yhat_hist )\n",
    "# model_hist_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model\n",
    "# model_xgb = xgb.XGBClassifier( n_jobs=-1 ).fit( X_train, y_train )\n",
    "\n",
    "# # prediction\n",
    "# yhat_xgb = model_xgb.predict( X_val )\n",
    "\n",
    "# # performance\n",
    "# model_xgb_results = ml_error( 'XGBoost',  y_val, yhat_xgb )\n",
    "# model_xgb_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model\n",
    "# model_cb = CatBoostClassifier( verbose=False ).fit( X_train, y_train )\n",
    "\n",
    "# # prediction\n",
    "# yhat_cb = model_cb.predict( X_val )\n",
    "\n",
    "# # performance\n",
    "# model_cb_results = ml_error( 'CatBoost',  y_val, yhat_cb)\n",
    "# model_cb_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Criando a rede neural\n",
    "# mlp_model = MLPClassifier().fit(X_train, y_train)\n",
    "\n",
    "# # Fit vai fazer o ajuste dos pesos (treinamento)\n",
    "# yhat_mlp = mlp_model.predict( X_val )\n",
    "\n",
    "# # Função que faz a contagem dos acertos e dos erros\n",
    "# model_mlp_results = ml_error( 'MLP',  y_val, yhat_mlp )\n",
    "# model_mlp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp_model = MLPClassifier().fit( X, Y )\n",
    "\n",
    "# pickle.dump( mlp_model, open( 'model/mlp_model.pkl', 'wb' ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model\n",
    "# model_et = ExtraTreesClassifier().fit( X_train, y_train )\n",
    "\n",
    "# # prediction\n",
    "# yhat_et = model_et.predict( X_val )\n",
    "\n",
    "# # performance\n",
    "# model_et_results = ml_error( 'Extra Trees',  y_val, yhat_et)\n",
    "# model_et_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.7 KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model\n",
    "# model_knn = KNeighborsClassifier().fit( X_train, y_train )\n",
    "\n",
    "# # prediction\n",
    "# yhat_knn = model_knn.predict( X_val )\n",
    "\n",
    "# # performance\n",
    "# model_knn_results = ml_error( 'KNN',  y_val, yhat_knn)\n",
    "# model_knn_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.8 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model\n",
    "# model_lr = LogisticRegression().fit( X_train, y_train )\n",
    "\n",
    "# # prediction\n",
    "# yhat_lr = model_lr.predict( X_val )\n",
    "\n",
    "# # performance\n",
    "# model_lr_results = ml_error( 'Logistic Regression',  y_val, yhat_lr)\n",
    "# model_lr_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.9 Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model\n",
    "# model_gaussian = GaussianNB().fit( X_train, y_train )\n",
    "\n",
    "# # prediction\n",
    "# yhat_gaussian = model_gaussian.predict( X_val )\n",
    "\n",
    "# # performance\n",
    "# model_gaussian_results = ml_error( 'Naive Bayes',  y_val, yhat_gaussian )\n",
    "# model_gaussian_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.10 SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model\n",
    "# model_svm = svm.SVC().fit( X_train, y_train )\n",
    "\n",
    "# # prediction\n",
    "# yhat_svm = model_svm.predict( X_val )\n",
    "\n",
    "# # performance\n",
    "# model_svm_results = ml_error( 'SVM',  y_val, yhat_svm )\n",
    "# model_svm_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.11 Stacking Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimators_list = [ ('lgbm', model_lgb), ('xgboost', model_xgb) ]\n",
    "\n",
    "# # Build stack model\n",
    "# stack_model = StackingClassifier( estimators = estimators_list, final_estimator=LogisticRegression(), n_jobs=-1, verbose=True ).fit( X_train, y_train )\n",
    "# pickle.dump( stack_model, open( 'model/model_stack.pkl', 'wb' ) )\n",
    "\n",
    "# # prediction\n",
    "# yhat_stack = stack_model.predict( X_val )\n",
    "\n",
    "# # performance\n",
    "# stack_model_results = ml_error( 'Stacking Model', y_val, yhat_stack )\n",
    "# stack_model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.0 Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search_space = [(100, 1500), #name = 'n_estimators'), \n",
    "#                 (1, 20), #name = 'max_depth'), \n",
    "#                 (0.001, 0.1, 'log-uniform'),#, name = 'learning_rate'),\n",
    "#                 (2, 128), #name = 'num_leaves'),\n",
    "#                 (1, 100), #name = 'min_child_samples'),\n",
    "#                 (0.05, 1.0), #name = 'subsample'),\n",
    "#                 (0.15, 1.0) #name = 'colsample_bytree')]\n",
    "# ]\n",
    "\n",
    "# def treinar_modelo( params ):\n",
    "#     n_estimators = params[0]\n",
    "#     max_depth = params[1]\n",
    "#     learning_rate = params[2]\n",
    "#     num_leaves = params[3]\n",
    "#     min_child_samples = params[4]\n",
    "#     subsample = params[5]\n",
    "#     colsample_bytree = params[6]\n",
    "\n",
    "#     print(params, '\\n')\n",
    "\n",
    "#     lgbm_model = lgb.LGBMClassifier(learning_rate = learning_rate, num_leaves=num_leaves, n_estimators=n_estimators, max_depth=max_depth, min_child_samples=min_child_samples, subsample=subsample, colsample_bytree=colsample_bytree, n_jobs=-1, random_state=42, subsample_freq=1)\n",
    "#     lgbm_model.fit( X_train, y_train )\n",
    "\n",
    "#     yhat_lgb = lgbm_model.predict( X_val )\n",
    "\n",
    "#     return -m.f1_score( y_val, yhat_lgb )\n",
    "\n",
    "# result = gp_minimize( treinar_modelo, search_space, n_calls = 200, n_initial_points = 10, verbose=True, n_jobs=-1, random_state= 42 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5e41ca046b3d9885ca897a71fa607c661abdd256ec5b789ecb59479222986451"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
